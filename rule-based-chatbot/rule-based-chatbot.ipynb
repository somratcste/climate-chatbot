{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import wordnet # to perform lemmatization\n",
    "from sklearn.feature_extraction.text import CountVectorizer # to perform bow\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\n",
    "from nltk import pos_tag # for parts of speech \n",
    "from sklearn.metrics import pairwise_distances # tor perform cosine similarity \n",
    "from nltk import word_tokenize # to create tokens\n",
    "from nltk.corpus import stopwords # for stop words \n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get question and nested answer \n",
    "data = []\n",
    "with open('data/climatology-1.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "def traverseAnswerObjects(answersObjects):\n",
    "    print(\"answer: \" + answersObjects[\"text\"])\n",
    "    print(\"\\n\")\n",
    "    if(len(answersObjects[\"answers\"])):\n",
    "        for answerObject in answersObjects[\"answers\"]:\n",
    "            traverseAnswerObjects(answerObject)\n",
    "\n",
    "for questionObject in data:\n",
    "    print(\"Question: \"+ questionObject[\"question\"])\n",
    "    print(\"\\n\")\n",
    "    traverseAnswerObjects(questionObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question and first answer \n",
    "# Get all json file from a directory\n",
    "path_to_json = 'data/'\n",
    "columns = ['question', 'answer']\n",
    "questionAndAnswerDf = pd.DataFrame(columns=columns)\n",
    "indexCount = 0\n",
    "for pos_json in os.listdir(path_to_json):\n",
    "    if pos_json.endswith('.json'):\n",
    "        with open(path_to_json + pos_json, encoding=\"utf8\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "            for index, questionObject in enumerate(data):\n",
    "                answer = []\n",
    "                if(len(questionObject[\"answers\"])):\n",
    "                    answer = questionObject[\"answers\"][0][\"text\"]\n",
    "                else :\n",
    "                    answer = questionObject[\"text\"]\n",
    "                questionAndAnswerDf.loc[indexCount] = [questionObject[\"question\"], answer]\n",
    "                indexCount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fills the null value with previous value\n",
    "questionAndAnswerDf.ffill(axis = 0, inplace = True)\n",
    "questionAndAnswerDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionAndAnswerDf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that performs text normalization steps \n",
    "def text_normalization(text):\n",
    "    text = str(text).lower() # text to lower case\n",
    "    spl_char_text = re.sub(r'[^a-z0-9]', ' ', text) # removing special characters\n",
    "    tokens = nltk.word_tokenize(spl_char_text) # word tokenizing\n",
    "    lema = wordnet.WordNetLemmatizer() # initializing lemmatization\n",
    "    tags_list = pos_tag(tokens, tagset = None) # parts of speech\n",
    "    lema_words = [] # empty list\n",
    "    for token,pos_token in tags_list:\n",
    "        if pos_token.startswith('V'): # verb\n",
    "            pos_val = 'v'\n",
    "        elif pos_token.startswith('J'): # adjective\n",
    "            pos_val = 'a'\n",
    "        elif pos_token.startswith('R'): # adverb\n",
    "            pos_val = 'r'\n",
    "        else: \n",
    "            pos_val = 'n' # noun\n",
    "        lema_token = lema.lemmatize(token, pos_val) # performing lemmatization\n",
    "        lema_words.append(lema_token) # appending the lemmatized token into a list \n",
    "    return \" \".join(lema_words) # returns the lemmatized tokens as a sentence\n",
    "text_normalization(\"going to play football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function to the dataset to get clean text\n",
    "questionAndAnswerDf[\"lemmatized_text\"] = questionAndAnswerDf[\"question\"].apply(text_normalization) \n",
    "questionAndAnswerDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow of words\n",
    "cv = CountVectorizer() # initializing the count vectorizer\n",
    "X = cv.fit_transform(questionAndAnswerDf[\"lemmatized_text\"]).toarray()\n",
    "\n",
    "# returns all the unique word from data\n",
    "features = cv.get_feature_names()\n",
    "questionAndAnswerDf_bou = pd.DataFrame(X, columns = features)\n",
    "questionAndAnswerDf_bou.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tf-idf\n",
    "tfidf = TfidfVectorizer() # intializing tf-id\n",
    "x_tfidf = tfidf.fit_transform(questionAndAnswerDf[\"lemmatized_text\"]).toarray() # transforming the data into array\n",
    "\n",
    "# returns all the unique word from data with a scroe of that word \n",
    "questionAndAnswerDf_tfidf = pd.DataFrame(x_tfidf, columns = tfidf.get_feature_names())\n",
    "questionAndAnswerDf_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greets = (\"hi\", \"hello\", \"good evening\", \"good afternoon\", \"hi there\",\n",
    "          \"good morning\", \"morning\", \"evening\", \"hey\", \"hey there\")\n",
    "\n",
    "identity_qs = (\"what are you\", \"who are you\")\n",
    "\n",
    "thanks = (\"thanks\", \"thank you\", \"thank you very much\", \"thank you so much\")\n",
    "\n",
    "farewells = (\"bye\", \"goodbye\", \"see ya\", \"see you\", \"cheers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that returns response to query\n",
    "def generate_response(text):\n",
    "    lemma = text_normalization(text) # text normalization\n",
    "    tf = tfidf.transform([lemma]).toarray() # apply tf-idf\n",
    "    cos = 1 - pairwise_distances(questionAndAnswerDf_tfidf, tf, metric = \"cosine\") # apply cosine similarity\n",
    "    index_value = cos.argmax() # getting index value\n",
    "    return questionAndAnswerDf[\"answer\"].loc[index_value][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat bot\n",
    "keep_dialogue_alive = True\n",
    "print(\"Hello, I am Captain Planet. Please ask me any question regarding climate change. üí™üåç\")\n",
    "while(keep_dialogue_alive):\n",
    "    human_txt = input().lower()\n",
    "    if human_txt not in farewells and human_txt not in thanks:\n",
    "        if human_txt in greets:\n",
    "            print(\"Captain Planet: \" + random.choice(greets))\n",
    "        elif human_txt in identity_qs:\n",
    "            print(\"Captain Planet: \"\n",
    "                  + \"I am a chatbot developed in a data science project \"\n",
    "                  + \"at the University of Bremen. I am here to answer your questions about climate change.\")\n",
    "        else:\n",
    "            print(\"Captain Planet: \" + generate_response(human_txt))\n",
    "    else:\n",
    "        keep_dialogue_alive = False\n",
    "        if human_txt in thanks:\n",
    "            print(\"Captain Planet: You're welcome!\")\n",
    "        else:\n",
    "            print(\"Captain Planet: Goodbye and thanks for your interest in climate change!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for server\n",
    "def final_response(text):\n",
    "    human_txt = text.decode('utf-8')\n",
    "    if human_txt not in farewells and human_txt not in thanks:\n",
    "        if human_txt in greets:\n",
    "            return \"Captain Planet: \" + random.choice(greets)\n",
    "        elif human_txt in identity_qs:\n",
    "            return \"Captain Planet: I am a chatbot developed in a data science project at the University of Bremen. I am here to answer your questions about climate change.\"\n",
    "        else:\n",
    "            return \"Captain Planet: \" + generate_response(human_txt)\n",
    "    else:\n",
    "        if human_txt in thanks:\n",
    "            return \"Captain Planet: You're welcome!\"\n",
    "        else:\n",
    "            return \"Captain Planet: Goodbye and thanks for your interest in climate change!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the server block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.server\n",
    "import socketserver\n",
    "\n",
    "PORT = 8080\n",
    "DIRECTORY = 'public'\n",
    "\n",
    "class Handler(http.server.SimpleHTTPRequestHandler):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, directory=DIRECTORY, **kwargs)\n",
    "\n",
    "    def do_POST(self):\n",
    "        self.send_response(200)\n",
    "        content_length = int(self.headers['Content-Length'])\n",
    "        post_body = self.rfile.read(content_length)\n",
    "        self.end_headers()\n",
    "        chatbot_reply = final_response(post_body)\n",
    "        self.wfile.write(str.encode(chatbot_reply))\n",
    "\n",
    "with socketserver.TCPServer(('', PORT), Handler) as httpd:\n",
    "    print('serving at port', PORT)\n",
    "    try:\n",
    "        httpd.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    httpd.server_close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
