{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Nazmul\n",
      "[nltk_data]     Hossain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Nazmul\n",
      "[nltk_data]     Hossain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nazmul Hossain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nazmul\n",
      "[nltk_data]     Hossain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import wordnet # to perform lemmatization\n",
    "from sklearn.feature_extraction.text import CountVectorizer # to perform bow\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\n",
    "from nltk import pos_tag # for parts of speech \n",
    "from sklearn.metrics import pairwise_distances # tor perform cosine similarity \n",
    "from nltk import word_tokenize # to create tokens\n",
    "from nltk.corpus import stopwords # for stop words \n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get question and nested answer \n",
    "data = []\n",
    "with open('data/climatology-1.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "def traverseAnswerObjects(answersObjects):\n",
    "    print(\"answer: \" + answersObjects[\"text\"])\n",
    "    print(\"\\n\")\n",
    "    if(len(answersObjects[\"answers\"])):\n",
    "        for answerObject in answersObjects[\"answers\"]:\n",
    "            traverseAnswerObjects(answerObject)\n",
    "\n",
    "for questionObject in data:\n",
    "    print(\"Question: \"+ questionObject[\"question\"])\n",
    "    print(\"\\n\")\n",
    "    traverseAnswerObjects(questionObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question and first answer \n",
    "# Get all json file from a directory\n",
    "path_to_json = 'data/'\n",
    "columns = ['question', 'answer']\n",
    "questionAndAnswerDf = pd.DataFrame(columns=columns)\n",
    "for pos_json in os.listdir(path_to_json):\n",
    "    if pos_json.endswith('.json'):\n",
    "        with open(path_to_json + pos_json, encoding=\"utf8\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "            for index, questionObject in enumerate(data):\n",
    "                answer = []\n",
    "                if(len(questionObject[\"answers\"])):\n",
    "                    answer = questionObject[\"answers\"][0][\"text\"]\n",
    "                else :\n",
    "                    answer = questionObject[\"text\"]\n",
    "                questionAndAnswerDf.loc[index] = [questionObject[\"question\"], answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Climate's changed before</td>\n",
       "      <td>Greenhouse gasses – mainly CO2, but also metha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>It's the sun</td>\n",
       "      <td>Over the last 35 years the sun has shown a coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>It's not bad</td>\n",
       "      <td>Here’s a list of cause and effect relationship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>There is no consensus</td>\n",
       "      <td>Science achieves a consensus when scientists s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>It's cooling</td>\n",
       "      <td>When looking for evidence of global warming, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   question                                             answer\n",
       "0  Climate's changed before  Greenhouse gasses – mainly CO2, but also metha...\n",
       "1              It's the sun  Over the last 35 years the sun has shown a coo...\n",
       "2              It's not bad  Here’s a list of cause and effect relationship...\n",
       "3     There is no consensus  Science achieves a consensus when scientists s...\n",
       "4              It's cooling  When looking for evidence of global warming, t..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fills the null value with previous value\n",
    "questionAndAnswerDf.ffill(axis = 0, inplace = True)\n",
    "questionAndAnswerDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionAndAnswerDf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go to play football'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function that performs text normalization steps \n",
    "def text_normalization(text):\n",
    "    text = str(text).lower() # text to lower case\n",
    "    spl_char_text = re.sub(r'[^a-z0-9]', ' ', text) # removing special characters\n",
    "    tokens = nltk.word_tokenize(spl_char_text) # word tokenizing\n",
    "    lema = wordnet.WordNetLemmatizer() # initializing lemmatization\n",
    "    tags_list = pos_tag(tokens, tagset = None) # parts of speech\n",
    "    lema_words = [] # empty list\n",
    "    for token,pos_token in tags_list:\n",
    "        if pos_token.startswith('V'): # verb\n",
    "            pos_val = 'v'\n",
    "        elif pos_token.startswith('J'): # adjective\n",
    "            pos_val = 'a'\n",
    "        elif pos_token.startswith('R'): # adverb\n",
    "            pos_val = 'r'\n",
    "        else: \n",
    "            pos_val = 'n' # noun\n",
    "        lema_token = lema.lemmatize(token, pos_val) # performing lemmatization\n",
    "        lema_words.append(lema_token) # appending the lemmatized token into a list \n",
    "    return \" \".join(lema_words) # returns the lemmatized tokens as a sentence\n",
    "text_normalization(\"going to play football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Climate's changed before</td>\n",
       "      <td>Greenhouse gasses – mainly CO2, but also metha...</td>\n",
       "      <td>climate s change before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>It's the sun</td>\n",
       "      <td>Over the last 35 years the sun has shown a coo...</td>\n",
       "      <td>it s the sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>It's not bad</td>\n",
       "      <td>Here’s a list of cause and effect relationship...</td>\n",
       "      <td>it s not bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>There is no consensus</td>\n",
       "      <td>Science achieves a consensus when scientists s...</td>\n",
       "      <td>there be no consensus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>It's cooling</td>\n",
       "      <td>When looking for evidence of global warming, t...</td>\n",
       "      <td>it s cool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   question  \\\n",
       "0  Climate's changed before   \n",
       "1              It's the sun   \n",
       "2              It's not bad   \n",
       "3     There is no consensus   \n",
       "4              It's cooling   \n",
       "\n",
       "                                              answer          lemmatized_text  \n",
       "0  Greenhouse gasses – mainly CO2, but also metha...  climate s change before  \n",
       "1  Over the last 35 years the sun has shown a coo...             it s the sun  \n",
       "2  Here’s a list of cause and effect relationship...             it s not bad  \n",
       "3  Science achieves a consensus when scientists s...    there be no consensus  \n",
       "4  When looking for evidence of global warming, t...                it s cool  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying the function to the dataset to get clean text\n",
    "questionAndAnswerDf[\"lemmatized_text\"] = questionAndAnswerDf[\"question\"].apply(text_normalization) \n",
    "questionAndAnswerDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>1500</th>\n",
       "      <th>16</th>\n",
       "      <th>1860</th>\n",
       "      <th>1880</th>\n",
       "      <th>1910</th>\n",
       "      <th>1934</th>\n",
       "      <th>1940</th>\n",
       "      <th>1960</th>\n",
       "      <th>...</th>\n",
       "      <th>weather</th>\n",
       "      <th>west</th>\n",
       "      <th>when</th>\n",
       "      <th>will</th>\n",
       "      <th>win</th>\n",
       "      <th>winter</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 448 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  10  1500  16  1860  1880  1910  1934  1940  1960  ...  weather  west  \\\n",
       "0    0   0     0   0     0     0     0     0     0     0  ...        0     0   \n",
       "1    0   0     0   0     0     0     0     0     0     0  ...        0     0   \n",
       "2    0   0     0   0     0     0     0     0     0     0  ...        0     0   \n",
       "3    0   0     0   0     0     0     0     0     0     0  ...        0     0   \n",
       "4    0   0     0   0     0     0     0     0     0     0  ...        0     0   \n",
       "\n",
       "   when  will  win  winter  with  would  wrong  year  \n",
       "0     0     0    0       0     0      0      0     0  \n",
       "1     0     0    0       0     0      0      0     0  \n",
       "2     0     0    0       0     0      0      0     0  \n",
       "3     0     0    0       0     0      0      0     0  \n",
       "4     0     0    0       0     0      0      0     0  \n",
       "\n",
       "[5 rows x 448 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow of words\n",
    "cv = CountVectorizer() # initializing the count vectorizer\n",
    "X = cv.fit_transform(questionAndAnswerDf[\"lemmatized_text\"]).toarray()\n",
    "\n",
    "# returns all the unique word from data\n",
    "features = cv.get_feature_names()\n",
    "questionAndAnswerDf_bou = pd.DataFrame(X, columns = features)\n",
    "questionAndAnswerDf_bou.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the stop words we have\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "# an example to remove stop words \n",
    "question = \"Should total ground heat flux be\"\n",
    "\n",
    "# checking for stop words\n",
    "Q = []\n",
    "a = question.split()\n",
    "for i in a:\n",
    "    if i in stop:\n",
    "        continue\n",
    "    else:\n",
    "        Q.append(i)\n",
    "    b = \" \".join(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionLemma = text_normalization(b) # for text normalizing\n",
    "questionBow = cv.transform([questionLemma]).toarray() # apply bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.4472136 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.57735027],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine similarity for the above question we considered \n",
    "cosineValue = 1 - pairwise_distances(questionAndAnswerDf_bou, questionBow, metric = \"cosine\")\n",
    "(cosineValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Previous major global climate changes were glacial cycles that happened long before human civilization developed. The human species evolved during the last 2.5 million years. Our far distant ancestors survived through multiple gradual cycles of cold ice ages, but did not experience any previous \"hot ages.\"  We homo sapiens in our current form appeared only about 200,000 years ago. So our species has survived two ice ages. In each ice age global temperatures were colder by 4 °C. The warmest period ever experienced by early humans was about 1 °C warmer (global average) than today. That period occured between the two most recent ice ages, 120,000 years ago (Eemian). Over the next 100,000 years temperatures gradually decreased into a new ice age. During that colder period humans began to expand out of Africa and across the globe. Ever since the Eemian much cooler temperatures have been the norm. Image by John Garrett. Human civilization is roughly 12,000 years old, as defined by the start of permanent settlements and agriculture. Agriculture became established as the glaciers retreated from the last ice age. Modern society has developed entirely in our current geological epoch, the Holocene. Global temperatures haven\\'t varied by more than 1 °C since. There have been regional shifts in climate (Medieval Warm Period, Little Ice Age, etc), but since civilization began humans have never experienced a hotter global climate than now.  Going back further, over a million years or so, our pre-human predecessors experienced a series of long cold glacial cycles. Several short interglacial periods were as warm or slightly warmer than our current climate. For example, the climate 400 kyrs ago, was slightly warmer than now. But more typically for the last million years it\\'s been 4 to 8 °C colder. Each transition from warm to glacial ages and back took thousands of years, giving humans and prehumans many generations to adjust.  So, really, the climate hasn\\'t changed much since we settled into towns, invented plumbing, and started calling ourselves civilized. Since humans and our human ancestors have been on Earth, average global temperatures have never been 3 °C warmer than now. In the next 100 years our children will be the first people ever to experience that kind of climate. But, perhaps Mr. Perry is thinking he\\'d like to live in a climate eons ago, closer to when the Earth was formed. Digging way back in time, we know that Earth\\'s climate has certainly been very different than it is now: 2 billion years ago there was not even any oxygen in the atmosphere. 550 million years ago high CO2 levels caused extreme greenhouse conditions. Humans were not around to care; the most advanced life form at that time was a flatworm. Humans could not physically survive over most of the planet in the age of the dinosaurs (Cretaceous, 100-65 Myr ago). Only very small mammals were beginning to evolve. Global average temperatures were 10-12 °C hotter than today. Most places on land were so hot that humans would risk fatal heat stroke every summer. The geological record shows many ancient changes in climate, including massive ice ages, hot-house conditions, oxygen-free and acidic oceans, and massive extinction events. These changes happened millions of years before humans, most occurred before even primitive mammals, appeared on the scene. Previous climate changes were caused by orbital wobbles, solar fluctuations, and movement of continents. None of those effects are causing the current heating http://sks.to/past.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the text at the above index becomes the response for the question\n",
    "questionAndAnswerDf[\"answer\"].loc[195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tf-idf\n",
    "tfidf = TfidfVectorizer() # intializing tf-id\n",
    "x_tfidf = tfidf.fit_transform(questionAndAnswerDf[\"lemmatized_text\"]).toarray() # transforming the data into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns all the unique word from data with a scroe of that word \n",
    "questionAndAnswerDf_tfidf = pd.DataFrame(x_tfidf, columns = tfidf.get_feature_names())\n",
    "questionAndAnswerDf_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tfidf = tfidf.transform([questionLemma]).toarray() # applying tf - idf\n",
    "cos = 1 - pairwise_distances(questionAndAnswerDf_tfidf, question_tfidf, metric = \"cosine\") # apply cosine similarity\n",
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_value1 = cos.argmax() # returns the index number of highest value\n",
    "index_value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionAndAnswerDf[\"answer\"].loc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that returns response to query\n",
    "def generate_response(text):\n",
    "    lemma = text_normalization(text) # text normalization\n",
    "    tf = tfidf.transform([lemma]).toarray() # apply tf-idf\n",
    "    cos = 1 - pairwise_distances(questionAndAnswerDf_tfidf, tf, metric = \"cosine\") # apply cosine similarity\n",
    "    index_value = cos.argmax() # getting index value\n",
    "    print(index_value)\n",
    "    return questionAndAnswerDf[\"answer\"].loc[index_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(\"The sun getting hotter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
